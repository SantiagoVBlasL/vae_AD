{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "m765709",
   "metadata": {},
   "source": "# β-VAE (Beta-Variational Autoencoder) para Clasificación AD vs CN\n## Ejemplo Educativo (Toy Example)\n\n**Objetivo del notebook:** Comprender el pipeline completo β-VAE + clasificador clásico para distinguir entre pacientes con Alzheimer (AD) y controles sanos (CN) usando matrices de conectividad fMRI simuladas.\n\n### Estructura del notebook\n1. Generación de datos sintéticos (sin necesidad de descargar el dataset ADNI)\n2. Arquitectura del Encoder y el Decoder (CNN con GroupNorm)\n3. Función de pérdida ELBO / β-VAE\n4. Bucle de entrenamiento PyTorch (sin K-Fold, split 80/20)\n5. Extracción de representaciones latentes\n6. Clasificadores clásicos: Logistic Regression y SVM\n7. Visualizaciones: espacio latente (PCA / t-SNE) y curva ROC\n\n> **Nota pedagógica:** Este notebook es una versión simplificada del pipeline de investigación `scripts/run_vae_clf_ad_inference.py`. Se eliminaron: validación cruzada K-Fold, logging (WandB/MLflow), manejo de artefactos complejos y tests de permutación, para centrarse en la arquitectura fundamental.\n"
  },
  {
   "cell_type": "markdown",
   "id": "m856363",
   "metadata": {},
   "source": "## 0. Importaciones y configuración\n"
  },
  {
   "cell_type": "code",
   "id": "c303060",
   "metadata": {},
   "source": "# --- Instalación rápida (ejecutar solo una vez si faltan paquetes) ---\n# !pip install torch numpy matplotlib scikit-learn\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score, accuracy_score, roc_curve\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\n# Reproducibilidad global\nSEED = 42\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Dispositivo: {device}')\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "m261965",
   "metadata": {},
   "source": "## 1. Generación de Datos Sintéticos\n\nLos datos reales del estudio son **matrices de conectividad funcional** (fMRI), representadas como tensores de forma `(N_sujetos, C_canales, ROIs, ROIs)`.\n\nPara este ejemplo usamos **1 canal**: la **matriz de correlación de Pearson** entre regiones de interés (ROIs) del atlas AAL3.\n\n### Geometría del tensor\n\n| Dimensión | Descripción | Valor (toy) |\n|-----------|-------------|-------------|\n| N | Número de sujetos | 200 |\n| C | Canales (solo Pearson) | 1 |\n| H = W | ROIs × ROIs (imagen cuadrada) | 64 × 64 |\n\n### Simulación de señal biológica\n\nPara que el clasificador tenga algo que aprender, añadimos una **diferencia sistemática** entre AD y CN en un bloque de la matriz (simulando hipoperfusión en AD).\n"
  },
  {
   "cell_type": "code",
   "id": "c110568",
   "metadata": {},
   "source": "# ── Parámetros del dataset sintético ─────────────────────────────────────\nN_SUBJECTS = 200    # total de sujetos (100 AD + 100 CN)\nN_CHANNELS = 1      # solo matriz de Pearson\nIMG_SIZE   = 64     # tamaño de la imagen (ROIs x ROIs)\nLATENT_DIM = 32     # dimensión del espacio latente del VAE\n\n# ── Crear matrices simétricas de conectividad ─────────────────────────────\ndef make_connectivity_matrix(n, img_size, noise_level=0.3,\n                              signal_block=None, signal_strength=0.0):\n    \"\"\"\n    Genera n matrices de correlación simétricas sintéticas.\n    signal_block: (r0, r1, c0, c1) donde añadir señal diferencial.\n    signal_strength: magnitud de la diferencia entre grupos (negativa -> hipoperfusión).\n    \"\"\"\n    X = np.random.randn(n, img_size, img_size).astype(np.float32) * noise_level\n    # Hacer la matriz simétrica (como una correlación real)\n    X = (X + X.transpose(0, 2, 1)) / 2.0\n    # Diagonal = 1  (autocorrelación perfecta)\n    idx = np.arange(img_size)\n    X[:, idx, idx] = 1.0\n    if signal_block is not None and signal_strength != 0.0:\n        r0, r1, c0, c1 = signal_block\n        X[:, r0:r1, c0:c1] += signal_strength\n        X[:, c0:c1, r0:r1] += signal_strength  # mantener simetría\n    return X\n\n# Bloque que simula la región con hipoperfusión en AD\nAD_BLOCK = (10, 25, 10, 25)\n\nX_cn = make_connectivity_matrix(N_SUBJECTS // 2, IMG_SIZE)\nX_ad = make_connectivity_matrix(N_SUBJECTS // 2, IMG_SIZE,\n                                 signal_block=AD_BLOCK, signal_strength=-0.5)\n\n# Apilar y añadir dimensión de canal: (200, 1, 64, 64)\nX_all = np.concatenate([X_cn, X_ad], axis=0)[:, np.newaxis, :, :]\ny_all = np.array([0] * (N_SUBJECTS // 2) + [1] * (N_SUBJECTS // 2), dtype=np.int64)\n\nprint(f'Tensor de entrada: {X_all.shape}  (N, C, H, W)')\nprint(f'Etiquetas: {np.bincount(y_all)}  (CN=0, AD=1)')\n\n# ── Normalización por sujeto (Z-score) ───────────────────────────────────\nX_norm = (X_all - X_all.mean(axis=(1,2,3), keepdims=True)) / \\\n         (X_all.std(axis=(1,2,3), keepdims=True) + 1e-6)\n\n# ── Split 80 / 20 estratificado ───────────────────────────────────────────\nX_train_np, X_test_np, y_train_np, y_test_np = train_test_split(\n    X_norm, y_all, test_size=0.2, stratify=y_all, random_state=SEED\n)\n\nprint(f'\\nTrain: {X_train_np.shape[0]} sujetos  |  Test: {X_test_np.shape[0]} sujetos')\nprint(f'Train CN/AD: {np.bincount(y_train_np)}  |  Test CN/AD: {np.bincount(y_test_np)}')\n\n# ── DataLoaders ───────────────────────────────────────────────────────────\nBATCH_SIZE   = 32\ntrain_ds     = TensorDataset(torch.from_numpy(X_train_np))\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "m623082",
   "metadata": {},
   "source": "## 2. Arquitectura del β-VAE\n\n### 2.1 Visión general\n\nEl β-VAE es un **autoencoder variacional con restricción aumentada sobre el espacio latente**, útil para aprender representaciones *disentangled* de datos complejos.\n\n```\nEntrada x              Espacio latente z              Reconstrucción x̂\n(1 × 64 × 64) ──Enc──► (μ, log σ²) ──Reparam──► z ──Dec──► (1 × 64 × 64)\n```\n\n### 2.2 Encoder\n\nAplica convoluciones sucesivas para comprimir `(C, H, W)` en dos vectores:\n- **μ** (media): coordenada central en el espacio latente.\n- **log σ²** (log-varianza): incertidumbre de la representación.\n\n### 2.3 El Truco de Reparametrización\n\nPara poder calcular gradientes a través de una operación estocástica:\n\n$$z = \\mu + \\sigma \\odot \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0, I)$$\n\nEsto convierte el muestreo (no diferenciable) en una operación determinista sobre $\\varepsilon$, permitiendo que los gradientes fluyan hacia el Encoder.\n\n### 2.4 Decoder\n\nRecibe $z$ y lo descomprime con capas `ConvTranspose2d` hasta reconstruir la imagen original.\n"
  },
  {
   "cell_type": "code",
   "id": "c226770",
   "metadata": {},
   "source": "class BetaVAE(nn.Module):\n    \"\"\"\n    beta-VAE convolucional simplificado para matrices de conectividad fMRI.\n\n    Arquitectura:\n        Encoder: 3 bloques Conv2d  -> [mu, log sigma^2]\n        Decoder: FC -> 3 bloques ConvTranspose2d -> reconstruccion\n    \"\"\"\n\n    def __init__(self, in_channels: int = 1, latent_dim: int = 32, img_size: int = 64):\n        super().__init__()\n        self.latent_dim = latent_dim\n\n        # ── ENCODER ──────────────────────────────────────────────────────\n        # Cada bloque: Conv2d + GELU (activacion suave) + GroupNorm + Dropout2d\n        # GroupNorm: mas estable que BatchNorm con batch pequeno\n        self.encoder = nn.Sequential(\n            # Bloque 1: (1, 64, 64) -> (32, ~31, ~31)\n            nn.Conv2d(in_channels, 32, kernel_size=5, stride=2, padding=1),\n            nn.GELU(),\n            nn.GroupNorm(8, 32),\n            nn.Dropout2d(0.1),\n\n            # Bloque 2: (32, ~31, ~31) -> (64, ~15, ~15)\n            nn.Conv2d(32, 64, kernel_size=5, stride=2, padding=1),\n            nn.GELU(),\n            nn.GroupNorm(8, 64),\n            nn.Dropout2d(0.1),\n\n            # Bloque 3: (64, ~15, ~15) -> (128, ~7, ~7)\n            nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=1),\n            nn.GELU(),\n            nn.GroupNorm(8, 128),\n        )\n\n        # Calcular el tamano del feature map aplanado tras el encoder\n        with torch.no_grad():\n            dummy = torch.zeros(1, in_channels, img_size, img_size)\n            enc_out = self.encoder(dummy)\n            self._flat_size = enc_out.view(1, -1).shape[1]\n            self._enc_shape  = enc_out.shape[1:]  # (C, H, W) del feature map\n\n        # Capas lineales que producen mu y log sigma^2\n        self.fc_mu     = nn.Linear(self._flat_size, latent_dim)\n        self.fc_logvar = nn.Linear(self._flat_size, latent_dim)\n\n        # ── DECODER ──────────────────────────────────────────────────────\n        # FC que proyecta z de vuelta al tamano del feature map del encoder\n        self.dec_fc = nn.Linear(latent_dim, self._flat_size)\n\n        self.decoder = nn.Sequential(\n            # Bloque 1: (128, ~7, ~7) -> (64, ~15, ~15)\n            nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2, padding=1, output_padding=0),\n            nn.GELU(),\n            nn.GroupNorm(8, 64),\n            nn.Dropout2d(0.1),\n\n            # Bloque 2: (64, ~15, ~15) -> (32, ~31, ~31)\n            nn.ConvTranspose2d(64, 32, kernel_size=5, stride=2, padding=1, output_padding=0),\n            nn.GELU(),\n            nn.GroupNorm(8, 32),\n            nn.Dropout2d(0.1),\n\n            # Bloque 3: (32, ~31, ~31) -> (1, 64, 64)\n            nn.ConvTranspose2d(32, in_channels, kernel_size=5, stride=2, padding=1, output_padding=1),\n            # Tanh: salida en [-1,1], coherente con la normalizacion Z-score de entrada\n            nn.Tanh(),\n        )\n\n    def encode(self, x):\n        \"\"\"Encoder: imagen -> (mu, log sigma^2)\"\"\"\n        h = self.encoder(x)        # extrae features convolucionales\n        h = h.view(h.size(0), -1)  # aplanar (flatten)\n        return self.fc_mu(h), self.fc_logvar(h)\n\n    @staticmethod\n    def reparameterize(mu, logvar):\n        \"\"\"\n        Truco de reparametrizacion: z = mu + sigma * eps,  eps ~ N(0, I)\n\n        CLAVE para backprop:\n        - El muestreo 'eps ~ N(0,I)' no tiene gradiente.\n        - Al expresar z = mu + sigma*eps, los gradientes\n          fluyen hacia mu y logvar sin problema.\n        \"\"\"\n        std = torch.exp(0.5 * logvar)   # sigma = exp(log sigma^2 / 2)\n        eps = torch.randn_like(std)     # eps ~ N(0, I)\n        return mu + eps * std           # z ~ N(mu, sigma^2 * I)\n\n    def decode(self, z):\n        \"\"\"Decoder: z -> imagen reconstruida\"\"\"\n        h = self.dec_fc(z)                       # proyeccion FC\n        h = h.view(h.size(0), *self._enc_shape)  # reshape a feature map\n        return self.decoder(h)\n\n    def forward(self, x):\n        mu, logvar = self.encode(x)\n        z = self.reparameterize(mu, logvar)\n        recon_x = self.decode(z)\n        # Si hay desajuste de tamano (bordes por convoluciones), interpolamos\n        if recon_x.shape != x.shape:\n            recon_x = nn.functional.interpolate(\n                recon_x, size=x.shape[2:], mode='bilinear', align_corners=False\n            )\n        return recon_x, mu, logvar, z\n\n\n# Instanciar y mover al dispositivo\nmodel = BetaVAE(in_channels=N_CHANNELS, latent_dim=LATENT_DIM, img_size=IMG_SIZE).to(device)\nn_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f'Parametros entrenables: {n_params:,}')\nprint(model)\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "m526893",
   "metadata": {},
   "source": "## 3. Función de Pérdida: ELBO y β-VAE\n\n### 3.1 Objetivo variacional (ELBO)\n\nEl VAE maximiza la **Evidencia del Límite Inferior** (ELBO):\n\n$$\\mathcal{L}_{\\text{ELBO}} = \\underbrace{\\mathbb{E}_{q_\\phi(z|x)}\\left[\\log p_\\theta(x|z)\\right]}_{\\text{Reconstrucción}} - \\underbrace{D_{\\text{KL}}\\left(q_\\phi(z|x) \\| p(z)\\right)}_{\\text{Regularización KL}}$$\n\ndonde:\n- $q_\\phi(z|x) = \\mathcal{N}(z;\\,\\mu_\\phi(x), \\sigma^2_\\phi(x)\\,I)$ es el **encoder**.\n- $p_\\theta(x|z)$ es el **decoder** (verosimilitud de reconstrucción).\n- $p(z) = \\mathcal{N}(0, I)$ es la prior estándar.\n\n### 3.2 El β-VAE\n\nHiggins et al. (2017) propusieron añadir $\\beta > 1$ para fomentar representaciones *disentangled*:\n\n$$\\mathcal{L}_{\\beta\\text{-VAE}} = \\underbrace{\\mathbb{E}\\left[\\| x - \\hat{x} \\|^2_2\\right]}_{\\text{MSE}} + \\beta \\cdot \\underbrace{D_{\\text{KL}}\\left(q_\\phi(z|x) \\| p(z)\\right)}_{\\text{KL}}$$\n\nLa **Divergencia KL** tiene forma analítica para gaussianas:\n\n$$D_{\\text{KL}}\\left(\\mathcal{N}(\\mu, \\sigma^2) \\| \\mathcal{N}(0, 1)\\right) = -\\frac{1}{2} \\sum_{j=1}^{d} \\left(1 + \\log \\sigma_j^2 - \\mu_j^2 - \\sigma_j^2\\right)$$\n\n### 3.3 Interpretación de β\n\n| β | Efecto |\n|---|--------|\n| β = 0 | Autoencoder determinista (sin regularización) |\n| β = 1 | VAE estándar (equilibrio reconstrucción/KL) |\n| β > 1 | **β-VAE**: penaliza más la KL → espacio latente más compacto y organizado |\n"
  },
  {
   "cell_type": "code",
   "id": "c153413",
   "metadata": {},
   "source": "def beta_vae_loss(recon_x, x, mu, logvar, beta: float = 1.0):\n    \"\"\"\n    Funcion de perdida del beta-VAE.\n\n    Args:\n        recon_x : reconstruccion del decoder, shape (B, C, H, W)\n        x       : imagen original,            shape (B, C, H, W)\n        mu      : media del encoder,           shape (B, latent_dim)\n        logvar  : log-varianza del encoder,    shape (B, latent_dim)\n        beta    : hiperparametro de regularizacion (beta=1 -> VAE clasico)\n    Returns:\n        (total_loss, recon_loss, kld_loss) como escalares\n    \"\"\"\n    # Perdida de Reconstruccion: MSE promediado por muestra\n    # sum() sobre pixeles, mean() (via /B) sobre el batch -> escala consistente\n    recon_loss = nn.functional.mse_loss(recon_x, x, reduction='sum') / x.size(0)\n\n    # Divergencia KL: forma analitica para gaussianas\n    # -0.5 * sum_j (1 + log sigma_j^2 - mu_j^2 - sigma_j^2)\n    kld_loss = -0.5 * torch.sum(\n        1 + logvar - mu.pow(2) - logvar.exp(), dim=1\n    ).mean()\n\n    total_loss = recon_loss + beta * kld_loss\n    return total_loss, recon_loss.detach(), kld_loss.detach()\n\n\n# Hiperparametros\nBETA         = 4.0    # beta > 1 fomenta disentanglement\nLR           = 1e-3\nEPOCHS       = 50\nWEIGHT_DECAY = 1e-4\n\noptimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n# Scheduler coseno: reduce LR suavemente hacia el final\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-5)\n\nprint(f'beta = {BETA}  |  LR = {LR}  |  Epocas = {EPOCHS}')\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "m584794",
   "metadata": {},
   "source": "## 4. Entrenamiento del β-VAE\n\nEl bucle de entrenamiento sigue el ciclo estándar de PyTorch:\n\n1. **Forward pass:** batch → `(recon_x, μ, log σ², z)`.\n2. **Calcular pérdida:** ELBO con β.\n3. **Backward pass:** gradientes con `.backward()`.\n4. **Optimizer step:** actualizar pesos con AdamW.\n\n> Sin K-Fold aquí: el modelo se entrena una sola vez sobre el 80% de los datos.\n"
  },
  {
   "cell_type": "code",
   "id": "c487675",
   "metadata": {},
   "source": "history = {'train_loss': [], 'train_recon': [], 'train_kld': []}\n\nprint(f\"{'Epoch':>6}  {'Train Loss':>12}  {'Recon':>10}  {'KLD':>10}  {'LR':>10}\")\nprint('-' * 60)\n\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    ep_loss, ep_recon, ep_kld = 0.0, 0.0, 0.0\n\n    for (batch,) in train_loader:\n        batch = batch.to(device)\n\n        optimizer.zero_grad(set_to_none=True)   # limpiar gradientes acumulados\n\n        # Forward: encoder -> reparametrize -> decoder\n        recon, mu, logvar, z = model(batch)\n\n        # Perdida beta-VAE\n        loss, recon_l, kld_l = beta_vae_loss(recon, batch, mu, logvar, beta=BETA)\n\n        loss.backward()  # calcular gradientes\n        # Gradient clipping: evita explosion de gradientes en las primeras epocas\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()  # actualizar pesos\n\n        n = batch.size(0)\n        ep_loss  += loss.item()    * n\n        ep_recon += recon_l.item() * n\n        ep_kld   += kld_l.item()   * n\n\n    scheduler.step()\n\n    N = len(train_loader.dataset)\n    history['train_loss'].append(ep_loss  / N)\n    history['train_recon'].append(ep_recon / N)\n    history['train_kld'].append(ep_kld   / N)\n\n    if epoch % 10 == 0 or epoch == 1:\n        lr_now = optimizer.param_groups[0]['lr']\n        print(f\"{epoch:>6}  {history['train_loss'][-1]:>12.4f}  \"\n              f\"{history['train_recon'][-1]:>10.4f}  \"\n              f\"{history['train_kld'][-1]:>10.4f}  \"\n              f\"{lr_now:>10.2e}\")\n\nprint('\\n Entrenamiento completado.')\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c305169",
   "metadata": {},
   "source": "# ── Curvas de aprendizaje ─────────────────────────────────────────────────\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\nspecs = [\n    ('train_loss',  'Perdida Total (beta * KL + Recon)', 'steelblue'),\n    ('train_recon', 'Perdida de Reconstruccion (MSE)',   'darkorange'),\n    ('train_kld',   'Divergencia KL',                    'mediumseagreen'),\n]\n\nfor ax, (key, title, color) in zip(axes, specs):\n    ax.plot(history[key], color=color, linewidth=2)\n    ax.set_title(title, fontsize=11)\n    ax.set_xlabel('Epoca')\n    ax.set_ylabel('Perdida')\n    ax.grid(True, alpha=0.3, linestyle='--')\n\nplt.suptitle(f'Historial de Entrenamiento del beta-VAE  (beta = {BETA})',\n             fontsize=13, y=1.02)\nplt.tight_layout()\nplt.savefig('betavae_training_curves.png', dpi=120, bbox_inches='tight')\nplt.show()\nprint('Curvas guardadas en betavae_training_curves.png')\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "m764597",
   "metadata": {},
   "source": "## 5. Extracción de Representaciones Latentes\n\nUna vez entrenado el VAE, usamos el **Encoder** para mapear cada sujeto a su vector latente **μ**.\n\n¿Por qué **μ** y no **z**?  \n- **z** incluye ruido del truco de reparametrización → variabilidad extra.  \n- **μ** es la estimación puntual de la media posterior → más estable para clasificación downstream.\n\n$$\\mu_{\\text{subj}} = \\text{Encoder}(x_{\\text{subj}}) \\in \\mathbb{R}^{d_z}$$\n"
  },
  {
   "cell_type": "code",
   "id": "c664707",
   "metadata": {},
   "source": "def extract_latents(model, X_np, batch_size=64):\n    \"\"\"Extrae los vectores mu del encoder para todos los sujetos.\"\"\"\n    model.eval()\n    mu_list = []\n    with torch.no_grad():\n        for i in range(0, len(X_np), batch_size):\n            batch = torch.from_numpy(X_np[i:i+batch_size]).to(device)\n            mu, _ = model.encode(batch)  # solo mu, descartamos logvar\n            mu_list.append(mu.cpu().numpy())\n    return np.concatenate(mu_list, axis=0)\n\n\nZ_train = extract_latents(model, X_train_np)\nZ_test  = extract_latents(model, X_test_np)\n\nprint(f'Latentes TRAIN: {Z_train.shape}  (N_train x latent_dim)')\nprint(f'Latentes TEST:  {Z_test.shape}   (N_test  x latent_dim)')\nprint(f'\\nMedias de mu (primeras 5 dims): {Z_train.mean(0)[:5].round(3)}')\nprint(f'Desv. estandar de mu (primeras 5 dims): {Z_train.std(0)[:5].round(3)}')\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "m193435",
   "metadata": {},
   "source": "## 6. Clasificadores Clásicos sobre el Espacio Latente\n\nEntrenamos dos clasificadores sobre los vectores μ:\n\n1. **Regresión Logística (LogReg):** modelo lineal, interpretable.\n   $$P(AD | \\mu) = \\sigma(W \\cdot \\mu + b), \\quad \\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n\n2. **Support Vector Machine (SVM-RBF):** no lineal con kernel RBF, útil cuando las clases no son linealmente separables en el espacio latente.\n\n### Escalado previo\n\nLos clasificadores sensibles a distancias necesitan features en la misma escala. Aplicamos `StandardScaler` ajustado **solo** con datos de entrenamiento (para evitar data leakage).\n"
  },
  {
   "cell_type": "code",
   "id": "c276313",
   "metadata": {},
   "source": "# ── Escalar representaciones latentes ────────────────────────────────────\nscaler = StandardScaler()\nZ_train_sc = scaler.fit_transform(Z_train)  # fit + transform en TRAIN\nZ_test_sc  = scaler.transform(Z_test)        # solo transform en TEST\n\n# ── Regresion Logistica ───────────────────────────────────────────────────\nlogreg = LogisticRegression(C=0.1, solver='liblinear', max_iter=1000,\n                             random_state=SEED)\nlogreg.fit(Z_train_sc, y_train_np)\n\ny_prob_lr = logreg.predict_proba(Z_test_sc)[:, 1]\ny_pred_lr = logreg.predict(Z_test_sc)\n\nauc_lr = roc_auc_score(y_test_np, y_prob_lr)\nacc_lr = accuracy_score(y_test_np, y_pred_lr)\nprint(f'Logistic Regression  ->  AUC: {auc_lr:.3f}  |  Accuracy: {acc_lr:.3f}')\n\n# ── SVM con kernel RBF ────────────────────────────────────────────────────\nsvm_clf = SVC(kernel='rbf', C=1.0, gamma='scale', probability=True,\n              random_state=SEED)\nsvm_clf.fit(Z_train_sc, y_train_np)\n\ny_prob_sv = svm_clf.predict_proba(Z_test_sc)[:, 1]\ny_pred_sv = svm_clf.predict(Z_test_sc)\n\nauc_sv = roc_auc_score(y_test_np, y_prob_sv)\nacc_sv = accuracy_score(y_test_np, y_pred_sv)\nprint(f'SVM (RBF)            ->  AUC: {auc_sv:.3f}  |  Accuracy: {acc_sv:.3f}')\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "m486286",
   "metadata": {},
   "source": "## 7. Visualizaciones\n\n### 7.1 Espacio Latente (PCA y t-SNE)\n\nProyectamos los vectores μ a 2D para inspeccionar la separabilidad entre AD y CN:\n\n- **PCA:** proyección lineal global (rápida).\n- **t-SNE:** proyección no lineal que preserva estructura local (más informativa).\n\n### 7.2 Curva ROC\n\nMuestra el tradeoff entre **Sensibilidad** (TPR) y **1 - Especificidad** (FPR) para todos los umbrales de decisión. El **AUC** (Área Bajo la Curva) resume el rendimiento en un solo número (1.0 = perfecto, 0.5 = azar).\n"
  },
  {
   "cell_type": "code",
   "id": "c251390",
   "metadata": {},
   "source": "# ── Proyecciones 2D ──────────────────────────────────────────────────────\npca  = PCA(n_components=2, random_state=SEED)\nZ_pca = pca.fit_transform(Z_test_sc)\npca_var = pca.explained_variance_ratio_\n\ntsne = TSNE(n_components=2, perplexity=15, random_state=SEED,\n            max_iter=1000, verbose=0)\nZ_tsne = tsne.fit_transform(Z_test_sc)\n\n# ── Curvas ROC ────────────────────────────────────────────────────────────\nfpr_lr, tpr_lr, _ = roc_curve(y_test_np, y_prob_lr)\nfpr_sv, tpr_sv, _ = roc_curve(y_test_np, y_prob_sv)\n\n# ── Figura compuesta ──────────────────────────────────────────────────────\nfig = plt.figure(figsize=(18, 5))\ngs  = gridspec.GridSpec(1, 3, figure=fig)\n\ncolors  = {0: '#4c9be8', 1: '#e8604c'}\nlabels  = {0: 'CN (Control)', 1: 'AD (Alzheimer)'}\nmarkers = {0: 'o', 1: '^'}\n\n# Panel 1: PCA\nax1 = fig.add_subplot(gs[0])\nfor cls in [0, 1]:\n    mask = y_test_np == cls\n    ax1.scatter(Z_pca[mask, 0], Z_pca[mask, 1], c=colors[cls],\n                label=labels[cls], alpha=0.8, s=60, marker=markers[cls],\n                edgecolors='k', linewidths=0.4)\nax1.set_title(\n    f'PCA del Espacio Latente\\n'\n    f'(PC1={pca_var[0]:.1%}, PC2={pca_var[1]:.1%})', fontsize=11\n)\nax1.set_xlabel('PC 1'); ax1.set_ylabel('PC 2')\nax1.legend(fontsize=9); ax1.grid(True, alpha=0.3, linestyle='--')\n\n# Panel 2: t-SNE\nax2 = fig.add_subplot(gs[1])\nfor cls in [0, 1]:\n    mask = y_test_np == cls\n    ax2.scatter(Z_tsne[mask, 0], Z_tsne[mask, 1], c=colors[cls],\n                label=labels[cls], alpha=0.8, s=60, marker=markers[cls],\n                edgecolors='k', linewidths=0.4)\nax2.set_title('t-SNE del Espacio Latente', fontsize=11)\nax2.set_xlabel('t-SNE 1'); ax2.set_ylabel('t-SNE 2')\nax2.legend(fontsize=9); ax2.grid(True, alpha=0.3, linestyle='--')\n\n# Panel 3: ROC\nax3 = fig.add_subplot(gs[2])\nax3.plot(fpr_lr, tpr_lr, lw=2, color='steelblue',\n         label=f'LogReg  (AUC = {auc_lr:.2f})')\nax3.plot(fpr_sv, tpr_sv, lw=2, color='darkorange',\n         label=f'SVM-RBF (AUC = {auc_sv:.2f})')\nax3.plot([0, 1], [0, 1], 'k--', lw=1.2, label='Azar (AUC = 0.5)')\nax3.fill_between(fpr_lr, tpr_lr, alpha=0.08, color='steelblue')\nax3.fill_between(fpr_sv, tpr_sv, alpha=0.08, color='darkorange')\nax3.set_title('Curva ROC - Test Set', fontsize=11)\nax3.set_xlabel('Tasa de Falsos Positivos (1 - Especificidad)')\nax3.set_ylabel('Tasa de Verdaderos Positivos (Sensibilidad)')\nax3.legend(fontsize=9); ax3.grid(True, alpha=0.3, linestyle='--')\nax3.set_xlim([0, 1]); ax3.set_ylim([0, 1.02])\n\nplt.suptitle(\n    'Evaluacion del Pipeline beta-VAE + Clasificador (Toy Example)',\n    fontsize=13, y=1.03\n)\nplt.tight_layout()\nplt.savefig('betavae_results.png', dpi=120, bbox_inches='tight')\nplt.show()\nprint('Figura guardada en betavae_results.png')\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "m270165",
   "metadata": {},
   "source": "## 8. Resumen y Puntos Clave\n\n### Pipeline completo\n\n```\nDatos brutos (1x64x64)\n      |\n      v\nNormalizacion (Z-score por sujeto)\n      |\n      v\nbeta-VAE Encoder: CNN -> mu, log sigma^2\n      |\n      +--- Truco de Reparametrizacion: z = mu + sigma * eps\n      |                    v\n      |              beta-VAE Decoder: z -> x_hat\n      |\n      v\nmu  (vector latente por sujeto, shape: latent_dim)\n      |\n      +--- StandardScaler\n      |\n      +--- Logistic Regression -> P(AD|mu)\n      +--- SVM-RBF             -> P(AD|mu)\n```\n\n### Conceptos clave\n\n| Concepto | Por qué importa |\n|----------|-----------------|\n| **β-VAE** | β > 1 fuerza un espacio latente más organizado (*disentangled*) |\n| **Truco de reparametrización** | Permite backprop a través de muestreo estocástico |\n| **μ vs z** | μ es más estable para clasificación (sin ruido de muestreo) |\n| **Escalar con train only** | Evita data leakage hacia el test set |\n| **AUC-ROC** | Métrica robusta a desbalance; independiente del umbral de decisión |\n\n### Extensiones posibles\n\n- Aumentar a **7 canales** (Pearson, MI, dFC, etc.) para aproximarse al paper original.\n- Reemplazar el split 80/20 por **K-Fold estratificado** para estimar varianza del AUC.\n- Agregar **LightGBM / XGBoost** como clasificadores adicionales.\n- Implementar **SHAP** para interpretar qué dimensiones latentes impulsan la clasificación.\n- Usar **programación cíclica de β** (β sube gradualmente en cada ciclo) para estabilizar el entrenamiento.\n\n### Referencias\n\n- Higgins, I. et al. (2017). *β-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework*. ICLR 2017.\n- Kingma, D. P. & Welling, M. (2013). *Auto-Encoding Variational Bayes*. arXiv:1312.6114.\n- Paper original de este repositorio: *Explainable Latent Representation Learning for Alzheimer's Disease: A β-VAE and Saliency Map Framework*.\n"
  }
 ]
}